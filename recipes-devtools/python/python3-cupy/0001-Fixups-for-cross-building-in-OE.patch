From aca6da1d8927fb6549f3fbc78c838687d03ca22b Mon Sep 17 00:00:00 2001
From: Ilies CHERGUI <ichergui@nvidia.com>
Date: Tue, 9 Dec 2025 14:04:41 +0000
Subject: [PATCH] Fixups for cross building in OE

Upstream-Status: Inappropriate [OE-specific]
Signed-off-by: Leo Fang <leof@nvidia.com>
Signed-off-by: Ilies CHERGUI <ichergui@nvidia.com>
---
 cupy/_core/core.pyx                           |   3 +-
 cupy/_core/dlpack.pxd                         |   2 +-
 .../_core/include/cupy/complex/math_private.h |  26 ++--
 cupy/_core/include/cupy/complex/namespace.h   |   7 +-
 cupy/_core/raw.pyx                            |  10 +-
 cupy/cuda/cub.pyx                             |  60 ----------
 cupy/cuda/cupy_cub.cu                         | 113 ++++++++----------
 cupy/cuda/cupy_cub.h                          |  13 +-
 cupy/cuda/cupy_jitify.h                       |   2 +-
 cupy/cuda/cupy_thrust.cu                      |  52 ++------
 cupy/cuda/jitify.pyx                          |   2 +-
 cupy/fft/_callback.pyx                        |   4 +-
 cupyx/scipy/sparse/_csr.py                    |  32 +----
 install/cupy_builder/_features.py             |  37 ++----
 install/cupy_builder/_preflight.py            |  43 -------
 install/cupy_builder/cupy_setup_build.py      |  36 +-----
 pyproject.toml                                |   7 +-
 tests/cupy_tests/core_tests/test_core.py      |   2 +-
 .../scipy_tests/sparse_tests/test_csr.py      |  42 -------
 19 files changed, 110 insertions(+), 383 deletions(-)

diff --git a/cupy/_core/core.pyx b/cupy/_core/core.pyx
index c9c903797..19d719e09 100644
--- a/cupy/_core/core.pyx
+++ b/cupy/_core/core.pyx
@@ -2294,12 +2294,13 @@ cdef inline str _translate_cucomplex_to_thrust(str source):
 
 cpdef tuple assemble_cupy_compiler_options(tuple options):
     for op in options:
+        # TODO(leofang): Update this. We need C++17 now.
         if '-std=c++' in op:
             if op.endswith('03'):
                 warnings.warn('CCCL requires c++11 or above')
             break
     else:
-        options += ('--std=c++11',)
+        options += ('--std=c++17',)
 
     # make sure bundled CCCL is searched first
     options = (_get_cccl_include_options()
diff --git a/cupy/_core/dlpack.pxd b/cupy/_core/dlpack.pxd
index 3f61fb79a..177658706 100644
--- a/cupy/_core/dlpack.pxd
+++ b/cupy/_core/dlpack.pxd
@@ -5,7 +5,7 @@ from libc.stdint cimport (
 )
 
 
-cdef extern from './include/cupy/_dlpack/dlpack.h' nogil:
+cdef extern from 'dlpack/dlpack.h' nogil:
     int DLPACK_MAJOR_VERSION
     int DLPACK_MINOR_VERSION
     int DLPACK_FLAG_BITMASK_READ_ONLY
diff --git a/cupy/_core/include/cupy/complex/math_private.h b/cupy/_core/include/cupy/complex/math_private.h
index c52c615e2..1372f0865 100644
--- a/cupy/_core/include/cupy/complex/math_private.h
+++ b/cupy/_core/include/cupy/complex/math_private.h
@@ -34,26 +34,24 @@
 
 #if defined(_MSC_VER)  // see #6823
 #include <cfloat>
+#else
+// from https://github.com/NVIDIA/cccl/blob/v3.1.2/libcudacxx/include/cuda/std/cfloat
+#  define FLT_MIN         1.17549435082228750796873653722224568e-38f
+#  define FLT_MAX         3.40282346638528859811704183484516925e+38f
+#  define FLT_EPSILON     1.19209289550781250000000000000000000e-7f
+#  define FLT_MAX_EXP     128
+#  define FLT_MANT_DIG    24
+#  define DBL_MIN         2.22507385850720138309023271733240406e-308
+#  define DBL_MAX         1.79769313486231570814527423731704357e+308
+#  define DBL_EPSILON     2.22044604925031308084726333618164062e-16
+#  define DBL_MAX_EXP     1024
+#  define DBL_MANT_DIG    53
 #endif  // defined(_MSC_VER)
 
 #include <cupy/complex/namespace.h>
 
 THRUST_NAMESPACE_BEGIN
 
-#if !defined(_MSC_VER)  // see #6823
-const float FLT_MIN = 1.17549435e-38F;
-const float FLT_MAX = 3.40282347e+38F;
-const float FLT_EPSILON = 1.19209290e-07F;
-const int FLT_MAX_EXP = 128;
-const int FLT_MANT_DIG = 24;
-
-const double DBL_MIN = 2.2250738585072014e-308;
-const double DBL_MAX = 1.7976931348623157e+308;
-const double DBL_EPSILON = 2.2204460492503131e-16;
-const int DBL_MAX_EXP = 1024;
-const int DBL_MANT_DIG = 53;
-#endif  // !defined(_MSC_VER)
-
 namespace detail {
 namespace complex {
 
diff --git a/cupy/_core/include/cupy/complex/namespace.h b/cupy/_core/include/cupy/complex/namespace.h
index f5e404adf..72801bed6 100644
--- a/cupy/_core/include/cupy/complex/namespace.h
+++ b/cupy/_core/include/cupy/complex/namespace.h
@@ -1,6 +1 @@
-#if defined(__CUDACC_RTC__) || defined(__HIPCC_RTC__) || ( defined(__HIPCC__) && HIP_VERSION < 50000000 )
-#define THRUST_NAMESPACE_BEGIN namespace thrust {
-#define THRUST_NAMESPACE_END }
-#else
-#include <thrust/detail/config.h>
-#endif
+#include <thrust/detail/config/namespace.h>
diff --git a/cupy/_core/raw.pyx b/cupy/_core/raw.pyx
index a6a980c86..82563129a 100644
--- a/cupy/_core/raw.pyx
+++ b/cupy/_core/raw.pyx
@@ -41,8 +41,8 @@ cdef class RawKernel:
             compile C++ kernels. Defaults to ``False``.
 
     .. note::
-        Starting CuPy v13.0.0, `RawKernel` by default compiles with the C++11
-        standard (``-std=c++11``) if it's not specified in ``options``.
+        Starting CuPy v14.0.0, `RawKernel` by default compiles with the C++17
+        standard (``-std=c++17``) if it's not specified in ``options``.
 
     .. _Jitify:
         https://github.com/NVIDIA/jitify
@@ -338,8 +338,8 @@ cdef class RawModule:
             compile C++ kernels. Defaults to ``False``.
 
     .. note::
-        Starting CuPy v13.0.0, `RawModule` by default compiles with the C++11
-        standard (``-std=c++11``) if it's not specified in ``options``.
+        Starting CuPy v14.0.0, `RawModule` by default compiles with the C++17
+        standard (``-std=c++17``) if it's not specified in ``options``.
 
     .. note::
         Each kernel in ``RawModule`` possesses independent function attributes.
@@ -449,7 +449,7 @@ cdef class RawModule:
                 '''
 
                 kers = ('func<int>', 'func<float>', 'func<double>')
-                mod = cupy.RawModule(code=code, options=('--std=c++11',),
+                mod = cupy.RawModule(code=code, options=('--std=c++17',),
                                      name_expressions=kers)
 
                 // retrieve func<int>
diff --git a/cupy/cuda/cub.pyx b/cupy/cuda/cub.pyx
index 8bded5978..5fd409cdd 100644
--- a/cupy/cuda/cub.pyx
+++ b/cupy/cuda/cub.pyx
@@ -55,8 +55,6 @@ cdef extern from 'cupy_cub.h' nogil:
                            int, int)
     void cub_device_segmented_reduce(void*, size_t&, void*, void*, int, int,
                                      Stream_t, int, int)
-    void cub_device_spmv(void*, size_t&, void*, void*, void*, void*, void*,
-                         int, int, int, Stream_t, int)
     void cub_device_scan(void*, size_t&, void*, void*, int, Stream_t, int, int)
     void cub_device_histogram_range(void*, size_t&, void*, void*, int, void*,
                                     size_t, Stream_t, int)
@@ -66,8 +64,6 @@ cdef extern from 'cupy_cub.h' nogil:
                                                 int, int)
     size_t cub_device_segmented_reduce_get_workspace_size(
         void*, void*, int, int, Stream_t, int, int)
-    size_t cub_device_spmv_get_workspace_size(
-        void*, void*, void*, void*, void*, int, int, int, Stream_t, int)
     size_t cub_device_scan_get_workspace_size(
         void*, void*, int, Stream_t, int, int)
     size_t cub_device_histogram_range_get_workspace_size(
@@ -275,62 +271,6 @@ def device_segmented_reduce(_ndarray_base x, op, tuple reduce_axis,
     return y
 
 
-def device_csrmv(int n_rows, int n_cols, int nnz, _ndarray_base values,
-                 _ndarray_base indptr, _ndarray_base indices, _ndarray_base x):
-    cdef _ndarray_base y
-    cdef memory.MemoryPointer ws
-    cdef void* values_ptr
-    cdef void* row_offsets_ptr
-    cdef void* col_indices_ptr
-    cdef void* x_ptr
-    cdef void* y_ptr
-    cdef void* ws_ptr
-    cdef int dtype_id
-    cdef size_t ws_size
-    cdef Stream_t s
-
-    if x.ndim != 1:
-        raise ValueError('array must be 1d')
-    if x.size != n_cols:
-        raise ValueError("size of array does not match the CSR matrix")
-    if runtime._is_hip_environment:
-        raise RuntimeError("hipCUB does not support SpMV")
-
-    if values.dtype == x.dtype:
-        dtype = values.dtype
-    else:
-        dtype = numpy.promote_types(values.dtype, x.dtype)
-        values = values.astype(dtype, "C", None, None, False)
-        x = x.astype(dtype, "C", None, None, False)
-
-    # CSR matrix attributes
-    values_ptr = <void*>values.data.ptr
-    row_offsets_ptr = <void*>indptr.data.ptr
-    col_indices_ptr = <void*>indices.data.ptr
-
-    x_ptr = <void*>x.data.ptr
-
-    # prepare output array
-    y = _core.ndarray((n_rows,), dtype=dtype)
-    y_ptr = <void*>y.data.ptr
-
-    s = <Stream_t>stream.get_current_stream_ptr()
-    dtype_id = common._get_dtype_id(dtype)
-
-    # get workspace size and then fire up
-    ws_size = cub_device_spmv_get_workspace_size(
-        values_ptr, row_offsets_ptr, col_indices_ptr, x_ptr, y_ptr, n_rows,
-        n_cols, nnz, s, dtype_id)
-    ws = memory.alloc(ws_size)
-    ws_ptr = <void *>ws.ptr
-    with nogil:
-        cub_device_spmv(ws_ptr, ws_size, values_ptr, row_offsets_ptr,
-                        col_indices_ptr, x_ptr, y_ptr, n_rows, n_cols, nnz, s,
-                        dtype_id)
-
-    return y
-
-
 def device_scan(_ndarray_base x, op):
     cdef memory.MemoryPointer ws
     cdef int dtype_id, x_size, op_code
diff --git a/cupy/cuda/cupy_cub.cu b/cupy/cuda/cupy_cub.cu
index 3036197c9..6df869055 100644
--- a/cupy/cuda/cupy_cub.cu
+++ b/cupy/cuda/cupy_cub.cu
@@ -2,16 +2,15 @@
 #include <cupy/type_dispatcher.cuh>
 
 #ifndef CUPY_USE_HIP
-#include <cfloat> // For FLT_MAX definitions
 #include <cub/device/device_reduce.cuh>
 #include <cub/device/device_segmented_reduce.cuh>
-#include <cub/device/device_spmv.cuh>
 #include <cub/device/device_scan.cuh>
 #include <cub/device/device_histogram.cuh>
-#include <cub/iterator/counting_input_iterator.cuh>
-#include <cub/iterator/transform_input_iterator.cuh>
+#include <thrust/iterator/counting_iterator.h>
+#include <thrust/iterator/transform_iterator.h>
 #include <cuda/functional>
 #include <cuda/std/functional>
+#include <cuda/std/limits> // numeric_limits
 #else
 #include <hipcub/device/device_reduce.hpp>
 #include <hipcub/device/device_segmented_reduce.hpp>
@@ -33,32 +32,60 @@
 using namespace cub;
 #define CUPY_CUB_NAMESPACE cub
 
+namespace cuda {
+
+namespace std {
+
 template <>
-struct FpLimits<complex<float>>
-{
-    static __host__ __device__ __forceinline__ complex<float> Max() {
-        return (complex<float>(FLT_MAX, FLT_MAX));
+class numeric_limits<thrust::complex<float>> {
+  public:
+    static __host__ __device__ thrust::complex<float> max() noexcept {
+        return thrust::complex<float>(cuda::std::numeric_limits<float>::max(), cuda::std::numeric_limits<float>::max());
     }
 
-    static __host__ __device__ __forceinline__ complex<float> Lowest() {
-        return (complex<float>(FLT_MAX * float(-1), FLT_MAX * float(-1)));
+    static __host__ __device__ thrust::complex<float> lowest() noexcept {
+        return thrust::complex<float>(-cuda::std::numeric_limits<float>::max(), -cuda::std::numeric_limits<float>::max());
+    }
+
+    static __host__ __device__ thrust::complex<float> infinity() noexcept {
+        return thrust::complex<float>(cuda::std::numeric_limits<float>::infinity(), cuda::std::numeric_limits<float>::infinity());
     }
+
+    static constexpr bool has_infinity = true;
+    static constexpr bool is_specialized = true;
 };
 
 template <>
-struct FpLimits<complex<double>>
-{
-    static __host__ __device__ __forceinline__ complex<double> Max() {
-        return (complex<double>(DBL_MAX, DBL_MAX));
+class numeric_limits<thrust::complex<double>> {
+  public:
+    static __host__ __device__ thrust::complex<double> max() noexcept {
+        return thrust::complex<double>(cuda::std::numeric_limits<double>::max(), cuda::std::numeric_limits<double>::max());
     }
 
-    static __host__ __device__ __forceinline__ complex<double> Lowest() {
-        return (complex<double>(DBL_MAX * double(-1), DBL_MAX * double(-1)));
+    static __host__ __device__ thrust::complex<double> lowest() noexcept {
+        return thrust::complex<double>(-cuda::std::numeric_limits<double>::max(), -cuda::std::numeric_limits<double>::max());
+    }
+
+    static __host__ __device__ thrust::complex<double> infinity() noexcept {
+        return thrust::complex<double>(cuda::std::numeric_limits<double>::infinity(), cuda::std::numeric_limits<double>::infinity());
     }
+
+    static constexpr bool has_infinity = true;
+    static constexpr bool is_specialized = true;
 };
 
-template <> struct NumericTraits<complex<float>>  : BaseTraits<FLOATING_POINT, true, false, unsigned int, complex<float>> {};
-template <> struct NumericTraits<complex<double>> : BaseTraits<FLOATING_POINT, true, false, unsigned long long, complex<double>> {};
+}  // namespace std
+
+template <>
+inline constexpr bool is_floating_point_v<thrust::complex<float>> = true;
+
+template <>
+inline constexpr bool is_floating_point_v<thrust::complex<double>> = true;
+
+}  // namespace cuda
+
+template <> struct NumericTraits<complex<float>>  : BaseTraits<FLOATING_POINT, true, unsigned int, thrust::complex<float>> {};
+template <> struct NumericTraits<complex<double>> : BaseTraits<FLOATING_POINT, true, unsigned long long, thrust::complex<double>> {};
 
 // need specializations for initial values
 namespace std {
@@ -113,7 +140,6 @@ class numeric_limits<__half> {
 
 }  // namespace std
 
-
 #else
 
 // hipCUB internally uses std::numeric_limits, so we should provide specializations for the complex numbers.
@@ -235,7 +261,7 @@ struct _arange
 };
 
 #ifndef CUPY_USE_HIP
-typedef TransformInputIterator<int, _arange, CountingInputIterator<int>> seg_offset_itr;
+typedef thrust::transform_iterator<_arange, thrust::counting_iterator<int>> seg_offset_itr;
 #else
 typedef TransformInputIterator<int, _arange, rocprim::counting_iterator<int>> seg_offset_itr;
 #endif
@@ -906,24 +932,6 @@ struct _cub_reduce_argmax {
 
 // TODO(leofang): add _cub_segmented_reduce_argmax
 
-//
-// **** CUB SpMV ****
-//
-struct _cub_device_spmv {
-    template <typename T>
-    void operator()(void* workspace, size_t& workspace_size, void* values,
-        void* row_offsets, void* column_indices, void* x, void* y,
-        int num_rows, int num_cols, int num_nonzeros, cudaStream_t stream)
-    {
-        #ifndef CUPY_USE_HIP
-        DeviceSpmv::CsrMV(workspace, workspace_size, static_cast<T*>(values),
-            static_cast<int*>(row_offsets), static_cast<int*>(column_indices),
-            static_cast<T*>(x), static_cast<T*>(y), num_rows, num_cols,
-            num_nonzeros, stream);
-        #endif
-    }
-};
-
 //
 // **** CUB InclusiveSum  ****
 //
@@ -1057,7 +1065,7 @@ void cub_device_segmented_reduce(void* workspace, size_t& workspace_size,
     // CUB internally use int for offset...
     // This iterates over [0, segment_size, 2*segment_size, 3*segment_size, ...]
     #ifndef CUPY_USE_HIP
-    CountingInputIterator<int> count_itr(0);
+    thrust::counting_iterator<int> count_itr(0);
     #else
     rocprim::counting_iterator<int> count_itr(0);
     #endif
@@ -1093,33 +1101,6 @@ size_t cub_device_segmented_reduce_get_workspace_size(void* x, void* y,
     return workspace_size;
 }
 
-/*--------- device spmv (sparse-matrix dense-vector multiply) ---------*/
-
-void cub_device_spmv(void* workspace, size_t& workspace_size, void* values,
-    void* row_offsets, void* column_indices, void* x, void* y, int num_rows,
-    int num_cols, int num_nonzeros, cudaStream_t stream,
-    int dtype_id)
-{
-    #ifndef CUPY_USE_HIP
-    return dtype_dispatcher(dtype_id, _cub_device_spmv(),
-                            workspace, workspace_size, values, row_offsets,
-                            column_indices, x, y, num_rows, num_cols,
-                            num_nonzeros, stream);
-    #endif
-}
-
-size_t cub_device_spmv_get_workspace_size(void* values, void* row_offsets,
-    void* column_indices, void* x, void* y, int num_rows, int num_cols,
-    int num_nonzeros, cudaStream_t stream, int dtype_id)
-{
-    size_t workspace_size = 0;
-    #ifndef CUPY_USE_HIP
-    cub_device_spmv(NULL, workspace_size, values, row_offsets, column_indices,
-                    x, y, num_rows, num_cols, num_nonzeros, stream, dtype_id);
-    #endif
-    return workspace_size;
-}
-
 /* -------- device scan -------- */
 
 void cub_device_scan(void* workspace, size_t& workspace_size, void* x, void* y,
diff --git a/cupy/cuda/cupy_cub.h b/cupy/cuda/cupy_cub.h
index 0eddad41a..d2f201dcf 100644
--- a/cupy/cuda/cupy_cub.h
+++ b/cupy/cuda/cupy_cub.h
@@ -27,13 +27,11 @@
 
 void cub_device_reduce(void*, size_t&, void*, void*, int, cudaStream_t, int, int);
 void cub_device_segmented_reduce(void*, size_t&, void*, void*, int, int, cudaStream_t, int, int);
-void cub_device_spmv(void*, size_t&, void*, void*, void*, void*, void*, int, int, int, cudaStream_t, int);
 void cub_device_scan(void*, size_t&, void*, void*, int, cudaStream_t, int, int);
 void cub_device_histogram_range(void*, size_t&, void*, void*, int, void*, size_t, cudaStream_t, int);
 void cub_device_histogram_even(void*, size_t&, void*, void*, int, int, int, size_t, cudaStream_t, int);
 size_t cub_device_reduce_get_workspace_size(void*, void*, int, cudaStream_t, int, int);
 size_t cub_device_segmented_reduce_get_workspace_size(void*, void*, int, int, cudaStream_t, int, int);
-size_t cub_device_spmv_get_workspace_size(void*, void*, void*, void*, void*, int, int, int, cudaStream_t, int);
 size_t cub_device_scan_get_workspace_size(void*, void*, int, cudaStream_t, int, int);
 size_t cub_device_histogram_range_get_workspace_size(void*, void*, int, void*, size_t, cudaStream_t, int);
 size_t cub_device_histogram_even_get_workspace_size(void*, void*, int, int, int, size_t, cudaStream_t, int);
@@ -44,6 +42,10 @@ __device__ long long atomicAdd(long long *address, long long val) {
     return atomicAdd(reinterpret_cast<unsigned long long*>(address),
                      static_cast<unsigned long long>(val));
 }
+__device__ long long atomicAdd_block(long long *address, long long val) {
+    return atomicAdd_block(reinterpret_cast<unsigned long long*>(address),
+                           static_cast<unsigned long long>(val));
+}
 #endif // __CUDA_ARCH__
 
 #if (defined(_MSC_VER) && (__CUDACC_VER_MAJOR__ == 11 && __CUDACC_VER_MINOR__ == 2))
@@ -60,9 +62,6 @@ void cub_device_reduce(...) {
 void cub_device_segmented_reduce(...) {
 }
 
-void cub_device_spmv(...) {
-}
-
 void cub_device_scan(...) {
 }
 
@@ -80,10 +79,6 @@ size_t cub_device_segmented_reduce_get_workspace_size(...) {
     return 0;
 }
 
-size_t cub_device_spmv_get_workspace_size(...) {
-    return 0;
-}
-
 size_t cub_device_scan_get_workspace_size(...) {
     return 0;
 }
diff --git a/cupy/cuda/cupy_jitify.h b/cupy/cuda/cupy_jitify.h
index d734e9fca..e444667c0 100644
--- a/cupy/cuda/cupy_jitify.h
+++ b/cupy/cuda/cupy_jitify.h
@@ -6,7 +6,7 @@
 
 #if !defined(CUPY_NO_CUDA) && !defined(CUPY_USE_HIP)
 
-#include <cupy/_jitify/jitify.hpp>
+#include <jitify.hpp>
 namespace jitify {
 namespace detail {
 const char* jitify_ver = _xstr_(CUPY_JITIFY_VERSION_CODE);
diff --git a/cupy/cuda/cupy_thrust.cu b/cupy/cuda/cupy_thrust.cu
index 3988bb06a..fdd659b84 100644
--- a/cupy/cuda/cupy_thrust.cu
+++ b/cupy/cuda/cupy_thrust.cu
@@ -9,20 +9,6 @@
 #include <thrust/tuple.h>
 #include <thrust/execution_policy.h>
 #include <type_traits>
-#if (__CUDACC_VER_MAJOR__ >11 || (__CUDACC_VER_MAJOR__ == 11 && __CUDACC_VER_MINOR__ >= 2) || HIP_VERSION >= 402)
-// This is used to avoid a problem with constexpr in functions declarations introduced in
-// cuda 11.2, MSVC 15 does not fully support it so we need a dummy constexpr declaration
-// that is provided by this header. However optional.h is only available
-// starting CUDA 10.1
-#include <thrust/optional.h>
-
-#ifdef _MSC_VER
-#define THRUST_OPTIONAL_CPP11_CONSTEXPR_LESS constexpr
-#else
-#define THRUST_OPTIONAL_CPP11_CONSTEXPR_LESS THRUST_OPTIONAL_CPP11_CONSTEXPR
-#endif
-
-#endif
 
 
 #if CUPY_USE_HIP
@@ -79,18 +65,6 @@ public:
     #define ENABLE_HALF
 #endif
 
-#if (__CUDACC_VER_MAJOR__ >11 || (__CUDACC_VER_MAJOR__ == 11 && __CUDACC_VER_MINOR__ >= 2))
-    #define CONSTEXPR_FUNC THRUST_OPTIONAL_CPP11_CONSTEXPR
-#else
-    #define CONSTEXPR_FUNC
-#endif
-
-#if (__CUDACC_VER_MAJOR__ >11 || (__CUDACC_VER_MAJOR__ == 11 && __CUDACC_VER_MINOR__ >= 2) || HIP_VERSION >= 402)
-    #define CONSTEXPR_COMPARATOR THRUST_OPTIONAL_CPP11_CONSTEXPR_LESS
-#else
-    #define CONSTEXPR_COMPARATOR
-#endif
-
 #ifdef ENABLE_HALF
 __host__ __device__ __forceinline__ bool isnan(const __half& x) {
     #if (defined(__CUDA_ARCH__) || defined(__HIP_DEVICE_COMPILE__))
@@ -102,7 +76,7 @@ __host__ __device__ __forceinline__ bool isnan(const __half& x) {
 #endif // ENABLE_HALF
 
 template <typename T>
-__host__ __device__ __forceinline__ CONSTEXPR_FUNC
+__host__ __device__ __forceinline__ constexpr
 static bool real_less(const T& lhs, const T& rhs) {
 #if  (defined(__CUDA_ARCH__) || defined(__HIP_DEVICE_COMPILE__))
     if (isnan(lhs)) {
@@ -118,7 +92,7 @@ static bool real_less(const T& lhs, const T& rhs) {
 }
 
 template <typename T>
-__host__ __device__ __forceinline__ CONSTEXPR_FUNC
+__host__ __device__ __forceinline__ constexpr
 static bool tuple_less(const thrust::tuple<size_t, T>& lhs,
 		 const thrust::tuple<size_t, T>& rhs) {
     const size_t& lhs_k = thrust::get<0>(lhs);
@@ -145,7 +119,7 @@ static bool tuple_less(const thrust::tuple<size_t, T>& lhs,
  */
 
 template <typename T>
-__host__ __device__ __forceinline__ CONSTEXPR_FUNC
+__host__ __device__ __forceinline__ constexpr
 static bool complex_less(const T& lhs, const T& rhs) {
     const bool lhsRe = isnan(lhs.real());
     const bool lhsIm = isnan(lhs.imag());
@@ -196,7 +170,7 @@ struct select_less {
 template <>
 struct select_less<complex<float>> {
     struct type {
-        __host__ __device__ __forceinline__ CONSTEXPR_COMPARATOR
+        __host__ __device__ __forceinline__ constexpr
         bool operator() (
             const complex<float>& lhs, const complex<float>& rhs) const {
             return complex_less(lhs, rhs);
@@ -207,7 +181,7 @@ struct select_less<complex<float>> {
 template <>
 struct select_less<complex<double>> {
     struct type {
-        __host__ __device__ __forceinline__ CONSTEXPR_COMPARATOR
+        __host__ __device__ __forceinline__ constexpr
         bool operator() (
             const complex<double>& lhs, const complex<double>& rhs) const {
             return complex_less(lhs, rhs);
@@ -218,7 +192,7 @@ struct select_less<complex<double>> {
 template <>
 struct select_less<thrust::tuple<size_t, complex<float>>> {
     struct type {
-        __host__ __device__ __forceinline__ CONSTEXPR_COMPARATOR
+        __host__ __device__ __forceinline__ constexpr
         bool operator() (
             const thrust::tuple<size_t, complex<float>>& lhs, const thrust::tuple<size_t, complex<float>>& rhs) const {
             return tuple_less(lhs, rhs);
@@ -229,7 +203,7 @@ struct select_less<thrust::tuple<size_t, complex<float>>> {
 template <>
 struct select_less<thrust::tuple<size_t, complex<double>>> {
     struct type {
-        __host__ __device__ __forceinline__ CONSTEXPR_COMPARATOR
+        __host__ __device__ __forceinline__ constexpr
         bool operator() (
             const thrust::tuple<size_t, complex<double>>& lhs, const thrust::tuple<size_t, complex<double>>& rhs) const {
             return tuple_less(lhs, rhs);
@@ -240,7 +214,7 @@ struct select_less<thrust::tuple<size_t, complex<double>>> {
 template <>
 struct select_less<thrust::tuple<size_t, float>> {
     struct type {
-        __host__ __device__ __forceinline__ CONSTEXPR_COMPARATOR
+        __host__ __device__ __forceinline__ constexpr
         bool operator() (
             const thrust::tuple<size_t, float>& lhs, const thrust::tuple<size_t, float>& rhs) const {
             return tuple_less(lhs, rhs);
@@ -251,7 +225,7 @@ struct select_less<thrust::tuple<size_t, float>> {
 template <>
 struct select_less<thrust::tuple<size_t, double>> {
     struct type {
-        __host__ __device__ __forceinline__ CONSTEXPR_COMPARATOR
+        __host__ __device__ __forceinline__ constexpr
         bool operator() (
             const thrust::tuple<size_t, double>& lhs, const thrust::tuple<size_t, double>& rhs) const {
             return tuple_less(lhs, rhs);
@@ -264,7 +238,7 @@ struct select_less<thrust::tuple<size_t, double>> {
 template <>
 struct select_less<float> {
     struct type {
-        __host__ __device__ __forceinline__ CONSTEXPR_COMPARATOR
+        __host__ __device__ __forceinline__ constexpr
         bool operator() (const float& lhs, const float& rhs) const {
             return real_less(lhs, rhs);
         }
@@ -274,7 +248,7 @@ struct select_less<float> {
 template <>
 struct select_less<double> {
     struct type {
-        __host__ __device__ __forceinline__ CONSTEXPR_COMPARATOR
+        __host__ __device__ __forceinline__ constexpr
         bool operator() (const double& lhs, const double& rhs) const {
             return real_less(lhs, rhs);
         }
@@ -285,7 +259,7 @@ struct select_less<double> {
 template <>
 struct select_less<__half> {
     struct type {
-        __host__ __device__ __forceinline__ CONSTEXPR_COMPARATOR
+        __host__ __device__ __forceinline__ constexpr
         bool operator() (const __half& lhs, const __half& rhs) const {
             return real_less(lhs, rhs);
         }
@@ -295,7 +269,7 @@ struct select_less<__half> {
 template <>
 struct select_less<thrust::tuple<size_t, __half>> {
     struct type {
-        __host__ __device__ __forceinline__ CONSTEXPR_COMPARATOR
+        __host__ __device__ __forceinline__ constexpr
         bool operator() (
             const thrust::tuple<size_t, __half>& lhs, const thrust::tuple<size_t, __half>& rhs) const {
 
diff --git a/cupy/cuda/jitify.pyx b/cupy/cuda/jitify.pyx
index 780d38bb2..bd92f0e4e 100644
--- a/cupy/cuda/jitify.pyx
+++ b/cupy/cuda/jitify.pyx
@@ -195,7 +195,7 @@ cdef inline void _init_cupy_headers_from_scratch() except*:
     # headers)
     # need to defer import to avoid circular dependency
     from cupy._core.core import assemble_cupy_compiler_options
-    cdef tuple options = ('-std=c++11', '-DCUB_DISABLE_BF16_SUPPORT',)
+    cdef tuple options = ('-std=c++17', '-DCUB_DISABLE_BF16_SUPPORT',)
     options = assemble_cupy_compiler_options(options)
     jitify(warmup_kernel, options)
 
diff --git a/cupy/fft/_callback.pyx b/cupy/fft/_callback.pyx
index 587bba15b..dbd0d1426 100644
--- a/cupy/fft/_callback.pyx
+++ b/cupy/fft/_callback.pyx
@@ -179,7 +179,7 @@ cdef inline void _mod_compile(str tempdir, str mod_name, str obj_host) except*:
                        '-I' + _python_include,
                        '-I' + _cuda_include,
                        '-I' + _cupy_include,
-                       '-fPIC', '-O2', '-std=c++11',
+                       '-fPIC', '-O2', '-std=c++17',
                        '-c', os.path.join(tempdir, mod_name + '.cpp'),
                        '-o', obj_host],
                        env=os.environ, cwd=tempdir)
@@ -259,7 +259,7 @@ cdef inline void _nvcc_compile(
     cmd = _nvcc + ['-arch=sm_'+arch, '-dc',
                    '-I' + _cupy_include,
                    '-c', os.path.join(tempdir, 'cupy_cufftXt.cu'),
-                   '-Xcompiler', '-fPIC', '-O2', '-std=c++11']
+                   '-Xcompiler', '-fPIC', '-O2', '-std=c++17']
     if cb_load:
         cmd.append('-DHAS_LOAD_CALLBACK')
     if cb_store:
diff --git a/cupyx/scipy/sparse/_csr.py b/cupyx/scipy/sparse/_csr.py
index 5dd270318..40dfd5d9d 100644
--- a/cupyx/scipy/sparse/_csr.py
+++ b/cupyx/scipy/sparse/_csr.py
@@ -1,3 +1,5 @@
+from __future__ import annotations
+
 import operator
 import warnings
 
@@ -10,8 +12,6 @@ except ImportError:
     _scipy_available = False
 
 import cupy
-from cupy._core import _accelerator
-from cupy.cuda import cub
 from cupy.cuda import runtime
 from cupyx.scipy.sparse import _base
 from cupyx.scipy.sparse import _compressed
@@ -188,20 +188,6 @@ class csr_matrix(_compressed._compressed_sparse_matrix):
             elif other.ndim == 1:
                 self.sum_duplicates()
                 other = cupy.asfortranarray(other)
-                # need extra padding to ensure not stepping on the CUB bug,
-                # see cupy/cupy#3679 for discussion
-                is_cub_safe = (self.indptr.data.mem.size
-                               > self.indptr.size * self.indptr.dtype.itemsize)
-                # CUB spmv is buggy since CUDA 11.0, see
-                # https://github.com/cupy/cupy/issues/3822#issuecomment-782607637
-                is_cub_safe &= (cub._get_cuda_build_version() < 11000)
-                for accelerator in _accelerator.get_routine_accelerators():
-                    if (accelerator == _accelerator.ACCELERATOR_CUB
-                            and not runtime.is_hip
-                            and is_cub_safe and other.flags.c_contiguous):
-                        return cub.device_csrmv(
-                            self.shape[0], self.shape[1], self.nnz,
-                            self.data, self.indptr, self.indices, other)
                 if (cusparse.check_availability('csrmvEx') and self.nnz > 0 and
                         cusparse.csrmvExIsAligned(self, other)):
                     # csrmvEx does not work if nnz == 0
@@ -295,25 +281,17 @@ class csr_matrix(_compressed._compressed_sparse_matrix):
 
     def _maximum_minimum(self, other, cupy_op, op_name, dense_check):
         if _util.isscalarlike(other):
-            other = cupy.asarray(other, dtype=self.dtype)
+            dtype = cupy.result_type(self.dtype, other)
+            other = cupy.asarray(other)
             if dense_check(other):
-                dtype = self.dtype
-                # Note: This is a work-around to make the output dtype the same
-                # as SciPy. It might be SciPy version dependent.
-                if dtype == numpy.float32:
-                    dtype = numpy.float64
-                elif dtype == numpy.complex64:
-                    dtype = numpy.complex128
-                dtype = cupy.result_type(dtype, other)
                 other = other.astype(dtype, copy=False)
-                # Note: The computation steps below are different from SciPy.
                 new_array = cupy_op(self.todense(), other)
                 return csr_matrix(new_array)
             else:
                 self.sum_duplicates()
                 new_data = cupy_op(self.data, other)
                 return csr_matrix((new_data, self.indices, self.indptr),
-                                  shape=self.shape, dtype=self.dtype)
+                                  shape=self.shape, dtype=dtype)
         elif _util.isdense(other):
             self.sum_duplicates()
             other = cupy.atleast_2d(other)
diff --git a/install/cupy_builder/_features.py b/install/cupy_builder/_features.py
index d97b9c07d..0c264c079 100644
--- a/install/cupy_builder/_features.py
+++ b/install/cupy_builder/_features.py
@@ -1,4 +1,5 @@
 import sys
+import os, re
 from typing import Any, Dict, List
 
 import cupy_builder.install_build as build
@@ -217,8 +218,6 @@ def get_features(ctx: Context) -> Dict[str, Feature]:
         'libraries': [
             'cudnn',
         ],
-        'check_method': build.check_cudnn_version,
-        'version_method': build.get_cudnn_version,
     }
     CUDA_nccl = {
         'name': 'nccl',
@@ -231,8 +230,6 @@ def get_features(ctx: Context) -> Dict[str, Feature]:
         'libraries': [
             'nccl',
         ],
-        'check_method': build.check_nccl_version,
-        'version_method': build.get_nccl_version,
     }
     CUDA_nvtx = {
         'name': 'nvtx',
@@ -269,12 +266,9 @@ def get_features(ctx: Context) -> Dict[str, Feature]:
             ('cupy.cuda.cub', ['cupy/cuda/cupy_cub.cu']),
         ],
         'include': [
-            'cub/util_namespace.cuh',  # dummy
         ],
         'libraries': list(_cudart_static_libs),
         'static_libraries': ['cudart_static'],
-        'check_method': build.check_cub_version,
-        'version_method': build.get_cub_version,
     }
     CUDA_jitify = {
         'name': 'jitify',
@@ -293,8 +287,6 @@ def get_features(ctx: Context) -> Dict[str, Feature]:
             'nvrtc',
         ] + _cudart_static_libs,
         'static_libraries': ['cudart_static'],
-        'check_method': build.check_jitify_version,
-        'version_method': build.get_jitify_version,
     }
     CUDA_random = {
         'name': 'random',
@@ -370,8 +362,6 @@ def get_features(ctx: Context) -> Dict[str, Feature]:
         'libraries': [
             'rccl',
         ],
-        'check_method': build.check_nccl_version,
-        'version_method': build.get_nccl_version,
     }
     HIP_thrust = {
         'name': 'thrust',
@@ -395,12 +385,9 @@ def get_features(ctx: Context) -> Dict[str, Feature]:
             ('cupy.cuda.thrust', ['cupy/cuda/cupy_thrust.cu']),
         ],
         'include': [
-            'thrust/version.h',
         ],
         'libraries': list(_cudart_static_libs),
         'static_libraries': ['cudart_static'],
-        'check_method': build.check_thrust_version,
-        'version_method': build.get_thrust_version,
     }
     COMMON_dlpack = {
         'name': 'dlpack',
@@ -409,7 +396,7 @@ def get_features(ctx: Context) -> Dict[str, Feature]:
             'cupy._core.dlpack',
         ],
         'include': [
-            'cupy/_dlpack/dlpack.h',
+            'dlpack/dlpack.h',
         ],
         'libraries': [],
     }
@@ -427,15 +414,12 @@ def get_features(ctx: Context) -> Dict[str, Feature]:
         features = [
             CUDA_cuda(ctx),
             _from_dict(CUDA_cusolver, ctx),
-            _from_dict(CUDA_cudnn, ctx),
             _from_dict(CUDA_nccl, ctx),
             _from_dict(CUDA_nvtx, ctx),
-            _from_dict(CUDA_cutensor, ctx),
             _from_dict(CUDA_cub, ctx),
             _from_dict(CUDA_jitify, ctx),
             _from_dict(CUDA_random, ctx),
             _from_dict(CUDA_thrust, ctx),
-            _from_dict(CUDA_cusparselt, ctx),
             _from_dict(COMMON_dlpack, ctx),
         ]
     return {f.name: f for f in features}
@@ -470,19 +454,18 @@ class CUDA_cuda(Feature):
 
     def configure(self, compiler: Any, settings: Any) -> bool:
         try:
-            out = build.build_and_run(compiler, '''
-            #include <cuda.h>
-            #include <stdio.h>
-            int main() {
-              printf("%d", CUDA_VERSION);
-              return 0;
-            }
-            ''', include_dirs=settings['include_dirs'])  # type: ignore[no-untyped-call] # NOQA
+            cuda_version_str = os.getenv('CUDA_VERSION')
+            match = re.match(r'^(\d+)\.(\d+)', cuda_version_str)
+            if match:
+                major, minor = match.groups()
+                out = 1000*int(major) + 10*int(minor)
+            else:
+                return False
         except Exception as e:
             utils.print_warning('Cannot check CUDA version', str(e))
             return False
 
-        self._version = int(out)
+        self._version = out
 
         if self._version < self.minimum_cuda_version:
             utils.print_warning(
diff --git a/install/cupy_builder/_preflight.py b/install/cupy_builder/_preflight.py
index ad5273bf0..6ef35a95c 100644
--- a/install/cupy_builder/_preflight.py
+++ b/install/cupy_builder/_preflight.py
@@ -5,47 +5,4 @@ from cupy_builder import Context
 
 
 def preflight_check(ctx: Context) -> bool:
-    if sys.platform not in ('linux', 'win32'):
-        print('Error: macOS is no longer supported', file=sys.stderr)
-        return False
-
-    source_root = ctx.source_root
-    is_git = os.path.isdir(os.path.join(source_root, '.git'))
-    for submodule in ('third_party/cccl',
-                      'third_party/jitify',
-                      'third_party/dlpack'):
-        dirpath = os.path.join(source_root, submodule)
-        if os.path.isdir(dirpath):
-            if 0 < len(os.listdir(dirpath)):
-                continue
-        else:
-            if not is_git:
-                # sdist does not contain third_party directory
-                continue
-
-        if is_git:
-            msg = f'''
-===========================================================================
-The directory {submodule} is a git submodule but is currently empty.
-Please use the command:
-
-    $ git submodule update --init
-
-to populate the directory before building from source.
-===========================================================================
-        '''
-        else:
-            msg = f'''
-===========================================================================
-The directory {submodule} is a git submodule but is currently empty.
-Instead of using ZIP/TAR archive downloaded from GitHub, use
-
-    $ git clone --recursive https://github.com/cupy/cupy.git
-
-to get a buildable CuPy source tree.
-===========================================================================
-        '''
-
-        print(msg, file=sys.stderr)
-        return False
     return True
diff --git a/install/cupy_builder/cupy_setup_build.py b/install/cupy_builder/cupy_setup_build.py
index 09b6fdb77..63ea9a809 100644
--- a/install/cupy_builder/cupy_setup_build.py
+++ b/install/cupy_builder/cupy_setup_build.py
@@ -217,10 +217,6 @@ def preconfigure_modules(ctx: Context, MODULES, compiler, settings):
             if module['name'] == 'cuda':
                 break
 
-    # Get a list of the CC of the devices connected to this node
-    if not ctx.use_hip:
-        build.check_compute_capabilities(compiler, settings)
-
     if len(ret) != len(MODULES):
         if 'cuda' in ret:
             lines = [
@@ -325,10 +321,6 @@ def make_extensions(ctx: Context, compiler, use_cython):
     settings['library_dirs'] = [
         x for x in settings['library_dirs'] if os.path.exists(x)]
 
-    # Adjust rpath to use CUDA libraries in `cupy/.data/lib/*.so`) from CuPy.
-    use_wheel_libs_rpath = (
-        0 < len(ctx.wheel_libs) and not PLATFORM_WIN32)
-
     # In the environment with CUDA 7.5 on Ubuntu 16.04, gcc5.3 does not
     # automatically deal with memcpy because string.h header file has
     # been changed. This is a workaround for that environment.
@@ -438,20 +430,9 @@ def make_extensions(ctx: Context, compiler, use_cython):
             if compiler.compiler_type == 'msvc':
                 compile_args.append('-D_USE_MATH_DEFINES')
 
-        if module['name'] == 'jitify':
-            # this fixes RTD (no_cuda) builds...
-            compile_args.append('--std=c++11')
-            # suppress printing Jitify logging to stdout
-            compile_args.append('-DJITIFY_PRINT_LOG=0')
-            # Uncomment to diagnose Jitify issues.
-            # compile_args.append('-DJITIFY_PRINT_ALL')
-
-            # if any change is made to the Jitify header, we force recompiling
-            s['depends'] = ['./cupy/_core/include/cupy/_jitify/jitify.hpp']
-
         if module['name'] == 'dlpack':
             # if any change is made to the DLPack header, we force recompiling
-            s['depends'] = ['./cupy/_core/include/cupy/_dlpack/dlpack.h']
+            s['depends'] = ['dlpack/dlpack.h']
 
         if module['name'] == 'numpy_allocator':
             # ensure the cdef public APIs have C linkage
@@ -465,21 +446,6 @@ def make_extensions(ctx: Context, compiler, use_cython):
                 continue
 
             rpath = []
-            if not ctx.no_rpath:
-                # Add library directories (e.g., `/usr/local/cuda/lib64`) to
-                # RPATH.
-                rpath += s_file['library_dirs']
-
-            if use_wheel_libs_rpath:
-                # Add `cupy/.data/lib` (where shared libraries included in
-                # wheels reside) to RPATH.
-                # The path is resolved relative to the module, e.g., use
-                # `$ORIGIN/../cupy/.data/lib` for `cupy/cudnn.so` and
-                # `$ORIGIN/../../../cupy/.data/lib` for
-                # `cupy_backends/cuda/libs/cudnn.so`.
-                depth = name.count('.')
-                rpath.append(
-                    '{}{}/cupy/.data/lib'.format(_rpath_base(), '/..' * depth))
 
             if (PLATFORM_LINUX and len(rpath) != 0):
                 ldflag = '-Wl,'
diff --git a/pyproject.toml b/pyproject.toml
index e60b1ac7b..653d9b30e 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -1,7 +1,7 @@
 [build-system]
 # Need to be installed manually when using `--no-build-isolation`.
 # Keep in sync with `docs/source/contribution.rst`.
-requires = ["setuptools>=77", "wheel", "Cython>=3,<3.2", "fastrlock>=0.5"]
+requires = ["setuptools>=69", "wheel", "Cython>=3,<3.2", "fastrlock>=0.5"]
 build-backend = "setuptools.build_meta"
 
 [project]
@@ -9,8 +9,8 @@ name = "cupy"
 description = "CuPy: NumPy & SciPy for GPU"
 authors = [{ name = "Seiya Tokui", email = "tokui@preferred.jp" }]
 maintainers = [{ name = "CuPy Developers" }]
-license = "MIT"
-license-files = ["LICENSE", "docs/source/license.rst"]
+license = { file = "LICENSE" }
+
 classifiers = [
   "Development Status :: 5 - Production/Stable",
   "Intended Audience :: Science/Research",
@@ -56,6 +56,7 @@ dynamic = [
 [tool.setuptools]
 # Do not include files listed in MANIFEST.in to wheels.
 include-package-data = false
+license-files = ["LICENSE", "docs/source/license.rst"]
 
 [tool.setuptools.packages.find]
 include = ["cupy*", "cupyx*", "cupy_backends*"]
diff --git a/tests/cupy_tests/core_tests/test_core.py b/tests/cupy_tests/core_tests/test_core.py
index 344f8e314..37ca8936e 100644
--- a/tests/cupy_tests/core_tests/test_core.py
+++ b/tests/cupy_tests/core_tests/test_core.py
@@ -95,7 +95,7 @@ class TestMinScalarType:
 
 
 @testing.parameterize(*testing.product({
-    'cxx': (None, '--std=c++11'),
+    'cxx': (None, '--std=c++17'),
 }))
 class TestCuPyHeaders(unittest.TestCase):
 
diff --git a/tests/cupyx_tests/scipy_tests/sparse_tests/test_csr.py b/tests/cupyx_tests/scipy_tests/sparse_tests/test_csr.py
index 36dca6295..ba37a2bf2 100644
--- a/tests/cupyx_tests/scipy_tests/sparse_tests/test_csr.py
+++ b/tests/cupyx_tests/scipy_tests/sparse_tests/test_csr.py
@@ -14,7 +14,6 @@ except ImportError:
 from cupy_backends.cuda.api import driver
 from cupy_backends.cuda.api import runtime
 import cupy
-from cupy._core import _accelerator
 from cupy import testing
 import cupyx.cusparse
 from cupyx.scipy import sparse
@@ -1789,47 +1788,6 @@ class TestCsrMatrixGetitem2:
         return _make(xp, sp, self.dtype)[None:4]
 
 
-# CUB SpMV works only when the matrix size is nonzero
-@testing.parameterize(*testing.product({
-    'make_method': ['_make', '_make_unordered', '_make_duplicate'],
-    'dtype': [numpy.float32, numpy.float64, cupy.complex64, cupy.complex128],
-}))
-@testing.with_requires('scipy')
-@pytest.mark.skipif(
-    cupy.cuda.cub._get_cuda_build_version() >= 11000,
-    reason='CUDA built-in CUB SpMV is buggy, see cupy/cupy#3822')
-@pytest.mark.skipif(runtime.is_hip, reason='hipCUB does not provide spmv')
-@pytest.mark.skipif(
-    not cupy.cuda.cub.available,
-    reason='The CUB routine is not enabled')
-class TestCubSpmv:
-
-    @pytest.fixture(autouse=True, scope='class')
-    def cub(self):
-        old_accelerators = _accelerator.get_routine_accelerators()
-        _accelerator.set_routine_accelerators(['cub'])
-        yield
-        _accelerator.set_routine_accelerators(old_accelerators)
-
-    @property
-    def make(self):
-        return globals()[self.make_method]
-
-    @testing.numpy_cupy_allclose(sp_name='sp')
-    def test_mul_dense_vector(self, xp, sp):
-        m = self.make(xp, sp, self.dtype)
-        x = xp.arange(4).astype(self.dtype)
-        if xp is numpy:
-            return m * x
-
-        # xp is cupy, first ensure we really use CUB
-        func = 'cupyx.scipy.sparse._csr.cub.device_csrmv'
-        with testing.AssertFunctionIsCalled(func):
-            m * x
-        # ...then perform the actual computation
-        return m * x
-
-
 @testing.parameterize(*testing.product({
     'a_dtype': ['float32', 'float64', 'complex64', 'complex128'],
     'b_dtype': ['float32', 'float64', 'complex64', 'complex128'],
-- 
2.34.1

